{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Installing Dependencies**"
      ],
      "metadata": {
        "id": "NVg2zwzhIQja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install catboost optuna"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9SRdfwsFAr5",
        "outputId": "92cc0891-0624-4c91-ae4a-ec78fe3f0986"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\n",
            "Requirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.11/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from catboost) (1.15.3)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.2.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Importing Necessary Libraries and Setup**"
      ],
      "metadata": {
        "id": "G7eOCdbzIc82"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.models import resnet18\n",
        "from torchvision import transforms\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "from typing import Tuple\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "from sklearn.metrics import roc_auc_score, roc_curve, precision_score, recall_score, f1_score\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier, early_stopping, log_evaluation\n",
        "from catboost import CatBoostClassifier\n",
        "import optuna\n",
        "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "# Warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3mAr1UgGZUo",
        "outputId": "3e4f407b-2cd1-4b70-9c0c-898e0de89cf9"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Defining Image Transform and Feature Extractor**"
      ],
      "metadata": {
        "id": "MRTcnmSuI0Ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalization stats for the dataset\n",
        "mean = [0.2980, 0.2962, 0.2987]\n",
        "std = [0.2886, 0.2875, 0.2889]\n",
        "\n",
        "class CustomTransform:\n",
        "    \"\"\"\n",
        "    Applies basic data augmentation, normalization, and moves image to the correct device.\n",
        "    \"\"\"\n",
        "    def __init__(self, mean, std):\n",
        "        self.to_tensor = transforms.ToTensor()\n",
        "        self.normalize = transforms.Normalize(mean=mean, std=std)\n",
        "        self.augment = transforms.Compose([\n",
        "            transforms.RandomHorizontalFlip(p=0.5),\n",
        "            transforms.ColorJitter(brightness=0.1, contrast=0.1)\n",
        "        ])\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if isinstance(img, torch.Tensor):\n",
        "            img_tensor = img\n",
        "        else:\n",
        "            img_tensor = self.to_tensor(self.augment(img))\n",
        "\n",
        "        if img_tensor.dim() == 3 and img_tensor.shape[0] not in [1, 3]:\n",
        "            img_tensor = img_tensor.permute(2, 0, 1)\n",
        "\n",
        "        return self.normalize(img_tensor).to(device)\n",
        "\n",
        "transform = CustomTransform(mean=mean, std=std)\n",
        "\n",
        "class FeatureExtractor(nn.Module):\n",
        "    \"\"\"\n",
        "    Extracts both conv5 features and final logits from ResNet18.\n",
        "    Used for creating input features for the attack model.\n",
        "    \"\"\"\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.features = nn.Sequential(*list(model.children())[:-2])\n",
        "        self.pool = model.avgpool\n",
        "        self.fc = model.fc\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_features = self.pool(self.features(x)).flatten(1)\n",
        "        logits = self.fc(conv_features)\n",
        "        return conv_features, logits"
      ],
      "metadata": {
        "id": "qwhfUM-ZGi4s"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Model and Define Datasets**"
      ],
      "metadata": {
        "id": "6OKBhGfgJDEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load victim model (ResNet18) with custom classification head and pretrained weights\n",
        "model = resnet18(weights=None)\n",
        "model.fc = nn.Linear(512, 44)\n",
        "ckpt = torch.load(\"./01_MIA.pt\", map_location=device)\n",
        "model.load_state_dict(ckpt)\n",
        "\n",
        "# Wrap the model to extract both conv5 features and logits\n",
        "extractor = FeatureExtractor(model).to(device)\n",
        "extractor.eval()\n",
        "\n",
        "# Dataset class to store IDs, images, and labels with optional transforms\n",
        "class TaskDataset(Dataset):\n",
        "    def __init__(self, transform=None):\n",
        "        self.ids = []\n",
        "        self.imgs = []\n",
        "        self.labels = []\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int]:\n",
        "        id_ = self.ids[index]\n",
        "        img = self.imgs[index]\n",
        "        if self.transform:\n",
        "            img = self.transform(img)\n",
        "        label = self.labels[index]\n",
        "        return id_, img, label\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.ids)\n",
        "\n",
        "# Extended dataset to include membership labels for MIA\n",
        "class MembershipDataset(TaskDataset):\n",
        "    def __init__(self, transform=None, is_private=False):\n",
        "        super().__init__(transform)\n",
        "        self.membership = []\n",
        "        self.is_private = is_private\n",
        "\n",
        "    def __getitem__(self, index) -> Tuple[int, torch.Tensor, int, int]:\n",
        "        id_, img, label = super().__getitem__(index)\n",
        "        if self.is_private:\n",
        "            return id_, img, label\n",
        "        return id_, img, label, self.membership[index]\n",
        "\n",
        "# Enable safe loading of MembershipDataset from torch.load\n",
        "torch.serialization.add_safe_globals({\"MembershipDataset\": MembershipDataset})\n",
        "\n",
        "# Load public (training) and private (test) datasets and assign appropriate flags and transforms\n",
        "public_data = torch.load(\"./pub.pt\", weights_only=False)\n",
        "priv_data = torch.load(\"./priv_out.pt\", weights_only=False)\n",
        "public_data.is_private = False\n",
        "priv_data.is_private = True\n",
        "public_data.transform = transform\n",
        "priv_data.transform = transform"
      ],
      "metadata": {
        "id": "wG7IvIdWGykS"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extraction and Engineering**"
      ],
      "metadata": {
        "id": "Ih-xcQHzJRDS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enhanced feature extraction from model outputs for MIA training\n",
        "def extract_features(extractor, imgs, labels):\n",
        "    \"\"\"\n",
        "    Extracts a comprehensive set of features from the victim model, including:\n",
        "    - Logits and softmax-based statistics (e.g. confidence, margin, entropy)\n",
        "    - Gradient-based metrics (e.g. norm, std)\n",
        "    - Internal conv5 representations\n",
        "    These are used to train a membership inference model.\n",
        "    \"\"\"\n",
        "    extractor.eval()\n",
        "    imgs = imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        conv_features, outputs = extractor(imgs)\n",
        "        probs = F.softmax(outputs, dim=1)\n",
        "\n",
        "        # Standard confidence/entropy-based features\n",
        "        confidences = torch.max(probs, dim=1)[0]\n",
        "        true_prob = probs[torch.arange(len(labels)), labels]\n",
        "        losses = F.cross_entropy(outputs, labels, reduction='none')\n",
        "        entropies = -torch.sum(probs * torch.log(probs + 1e-8), dim=1)\n",
        "\n",
        "        # Top-k softmax statistics\n",
        "        topk_probs, topk_indices = torch.topk(probs, k=5, dim=1)\n",
        "        margins = topk_probs[:, 0] - topk_probs[:, 1]\n",
        "        prob_diff = topk_probs[:, 0] - topk_probs[:, 2]\n",
        "        prob_ratio = topk_probs[:, 0] / (topk_probs[:, 1] + 1e-8)\n",
        "\n",
        "        # Logit-based features\n",
        "        true_logits = outputs[torch.arange(len(labels)), labels]\n",
        "        logit_norm = torch.norm(outputs, p=2, dim=1)\n",
        "        logit_std = torch.std(outputs, dim=1)\n",
        "        max_logit = torch.max(outputs, dim=1)[0]\n",
        "        diff_max_true_logit = max_logit - true_logits\n",
        "        logit_percentiles = torch.quantile(outputs, torch.tensor([0.25, 0.5, 0.75], device=device), dim=1)\n",
        "\n",
        "        # Softmax shape indicators\n",
        "        prob_skew = (probs - probs.mean(dim=1, keepdim=True)).pow(3).mean(dim=1) / (probs.std(dim=1).pow(3) + 1e-8)\n",
        "        prob_kurtosis = (probs - probs.mean(dim=1, keepdim=True)).pow(4).mean(dim=1) / (probs.std(dim=1).pow(4) + 1e-8) - 3\n",
        "        prob_max_min = topk_probs[:, 0] - topk_probs[:, -1]\n",
        "\n",
        "        # Top-k presence and relative comparison features\n",
        "        true_in_top3 = torch.any(topk_indices == labels.unsqueeze(1), dim=1).float()\n",
        "        true_in_top5 = torch.any(topk_indices == labels.unsqueeze(1), dim=1).float()\n",
        "        confidence_minus_true = confidences - true_prob\n",
        "        true_prob_confidence_ratio = true_prob / (confidences + 1e-8)\n",
        "\n",
        "    # Gradient-based features (backprop once per batch)\n",
        "    imgs.requires_grad_(True)\n",
        "    _, outputs = extractor(imgs)\n",
        "    loss = F.cross_entropy(outputs, labels)\n",
        "    loss.backward()\n",
        "    gradients = imgs.grad\n",
        "    gradient_norm = torch.norm(gradients, p=2, dim=(1, 2, 3))\n",
        "    gradient_mean = gradients.abs().mean(dim=(1, 2, 3))\n",
        "    gradient_std = gradients.std(dim=(1, 2, 3))\n",
        "    imgs.requires_grad_(False)\n",
        "\n",
        "    # Prediction correctness and probability variability\n",
        "    correct = (torch.argmax(outputs, dim=1) == labels).float()\n",
        "    prob_std = torch.std(probs, dim=1)\n",
        "\n",
        "    # Return all features in a dictionary\n",
        "    features = {\n",
        "        'loss': losses.cpu(),\n",
        "        'margin': margins.cpu(),\n",
        "        'true_logit': true_logits.cpu(),\n",
        "        'gradient_norm': gradient_norm.cpu(),\n",
        "        'gradient_mean': gradient_mean.cpu(),\n",
        "        'gradient_std': gradient_std.cpu(),\n",
        "        'confidence': confidences.cpu(),\n",
        "        'entropy': entropies.cpu(),\n",
        "        'correct': correct.cpu(),\n",
        "        'prob_std': prob_std.cpu(),\n",
        "        'top1_prob': topk_probs[:, 0].cpu(),\n",
        "        'top2_prob': topk_probs[:, 1].cpu(),\n",
        "        'top3_prob': topk_probs[:, 2].cpu(),\n",
        "        'top5_prob': topk_probs[:, 4].cpu(),\n",
        "        'true_prob': true_prob.cpu(),\n",
        "        'logit_norm': logit_norm.cpu(),\n",
        "        'logit_std': logit_std.cpu(),\n",
        "        'logit_p25': logit_percentiles[0].cpu(),\n",
        "        'logit_p50': logit_percentiles[1].cpu(),\n",
        "        'logit_p75': logit_percentiles[2].cpu(),\n",
        "        'diff_max_true_logit': diff_max_true_logit.cpu(),\n",
        "        'true_in_top3': true_in_top3.cpu(),\n",
        "        'true_in_top5': true_in_top5.cpu(),\n",
        "        'confidence_minus_true': confidence_minus_true.cpu(),\n",
        "        'true_prob_confidence_ratio': true_prob_confidence_ratio.cpu(),\n",
        "        'prob_diff': prob_diff.cpu(),\n",
        "        'prob_ratio': prob_ratio.cpu(),\n",
        "        'prob_skew': prob_skew.cpu(),\n",
        "        'prob_kurtosis': prob_kurtosis.cpu(),\n",
        "        'prob_max_min': prob_max_min.cpu(),\n",
        "        'conv_features': conv_features.cpu()\n",
        "    }\n",
        "    return features\n",
        "\n",
        "\n",
        "def add_derived_features(row, conv_features):\n",
        "    \"\"\"\n",
        "    Adds additional hand-engineered features and concatenates them with conv5 features.\n",
        "    These combine existing stats in ways that help the model identify subtle patterns.\n",
        "    \"\"\"\n",
        "    row = [row[k].item() for k in range(len(row))]\n",
        "    new_features = [\n",
        "        np.log(row[0] + 1e-5),\n",
        "        row[6] / (row[7] + 1e-5),\n",
        "        row[1] / (row[2] + 1e-5),\n",
        "        row[14] * row[6],\n",
        "        (row[3] + 1e-5) / (row[16] + 1e-5),\n",
        "        row[20] * row[8],\n",
        "        row[6] - row[14],\n",
        "        row[14] / (row[10] + 1e-5),\n",
        "        row[10] - row[11],\n",
        "        row[11] - row[12],\n",
        "        row[12] - row[13],\n",
        "        row[16] / (row[15] + 1e-5),\n",
        "        row[4] / (row[5] + 1e-5),\n",
        "        row[27] * row[28],\n",
        "        row[17] - row[19],\n",
        "        row[6] / (row[7] + 1e-5),\n",
        "        row[14] / (row[10] - row[14] + 1e-5),\n",
        "        row[3] * row[16],\n",
        "        abs(row[10] - row[14]),\n",
        "        (row[10] + row[11]) / (row[14] + 1e-5),\n",
        "        row[3] / (row[4] + 1e-5)\n",
        "    ]\n",
        "    return np.concatenate([row, new_features, conv_features])"
      ],
      "metadata": {
        "id": "R2rUJbkFG8ij"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Extraction from Public Data**"
      ],
      "metadata": {
        "id": "qSIlxrQmJXbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract feature vectors and membership labels from the public dataset\n",
        "public_loader = DataLoader(public_data, batch_size=64, shuffle=False)\n",
        "X_train, y_train = [], []\n",
        "\n",
        "for ids, imgs, labels, memberships in tqdm(public_loader, desc=\"Processing public data\"):\n",
        "    imgs = imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    features = extract_features(extractor, imgs, labels)\n",
        "\n",
        "    # Process each sample in the batch\n",
        "    for j in range(len(ids)):\n",
        "        # Combine selected features and conv5 features\n",
        "        row = [features[k][j] for k in features if k != 'conv_features']\n",
        "        row = add_derived_features(row, features['conv_features'][j].numpy())\n",
        "        X_train.append(row)\n",
        "        y_train.append(memberships[j].item())\n",
        "\n",
        "# Convert to NumPy arrays for model training\n",
        "X_train = np.array(X_train)\n",
        "y_train = np.array(y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dwN32vU0HNEp",
        "outputId": "ab881706-6c8a-4e23-b7c9-da7b01bc4d09"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing public data: 100%|██████████| 313/313 [00:19<00:00, 16.39it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Feature Selection and Preprocessing**"
      ],
      "metadata": {
        "id": "mNefsmHFJcOe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select top 50 most informative features using ANOVA F-score\n",
        "selector = SelectKBest(f_classif, k=50)\n",
        "X_train_selected = selector.fit_transform(X_train, y_train)\n",
        "selected_features = selector.get_support(indices=True)\n",
        "\n",
        "# Check class balance in the training labels\n",
        "print(\"Class distribution in y_train:\")\n",
        "print(pd.Series(y_train).value_counts(normalize=True))\n",
        "\n",
        "# Normalize selected features using quantile transformation to Gaussian distribution\n",
        "scaler = QuantileTransformer(output_distribution='normal', random_state=42)\n",
        "X_scaled = scaler.fit_transform(X_train_selected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw7PN9iAHVck",
        "outputId": "4e28d46c-86d5-4b88-86be-0456b2346d0f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Class distribution in y_train:\n",
            "0    0.5\n",
            "1    0.5\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation Functions**"
      ],
      "metadata": {
        "id": "W3no5BMnJoWU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_metrics(y_true, y_pred, y_score):\n",
        "    auc = roc_auc_score(y_true, y_score)\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "\n",
        "    results = {\n",
        "        'AUC': auc,\n",
        "        'TPR@FPR=0.05': tpr[np.argmin(np.abs(fpr - 0.05))],\n",
        "        'TPR@FPR=0.01': tpr[np.argmin(np.abs(fpr - 0.01))],\n",
        "        'Best Threshold': thresholds[np.argmax(tpr - fpr)],\n",
        "        'Accuracy': (y_pred == y_true).mean(),\n",
        "        'Precision': precision_score(y_true, y_pred),\n",
        "        'Recall': recall_score(y_true, y_pred),\n",
        "        'F1': f1_score(y_true, y_pred)\n",
        "    }\n",
        "    return results\n",
        "\n",
        "# Find decision threshold that gives the closest FPR to a target value (e.g., 0.05)\n",
        "def optimize_threshold(y_true, y_score, target_fpr=0.05):\n",
        "    fpr, tpr, thresholds = roc_curve(y_true, y_score)\n",
        "    idx = np.argmin(np.abs(fpr - target_fpr))\n",
        "    return thresholds[idx]"
      ],
      "metadata": {
        "id": "CZIFM0WNHdCE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Optuna Tuner for CatBoost**"
      ],
      "metadata": {
        "id": "hNWW7o-mJuec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def optimize_catboost(X_train, y_train, X_val, y_val):\n",
        "    \"\"\"\n",
        "    Tune CatBoost hyperparameters using Optuna to maximize AUC on validation data.\n",
        "    \"\"\"\n",
        "    def objective(trial):\n",
        "        # Define search space for hyperparameters\n",
        "        params = {\n",
        "            'iterations': trial.suggest_int('iterations', 500, 2000),  # Fewer iterations for speed\n",
        "            'depth': trial.suggest_int('depth', 4, 8),\n",
        "            'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.05, log=True),\n",
        "            'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 1, 5),\n",
        "            'random_strength': trial.suggest_float('random_strength', 0.1, 0.5),\n",
        "            'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
        "            'random_state': 42,\n",
        "            'verbose': 0,  # Keep training quiet\n",
        "            'eval_metric': 'AUC'\n",
        "        }\n",
        "\n",
        "        # Train and evaluate on validation set\n",
        "        model = CatBoostClassifier(**params)\n",
        "        model.fit(X_train, y_train, eval_set=(X_val, y_val), early_stopping_rounds=50, verbose=0)\n",
        "        best_score = model.get_best_score()\n",
        "        return best_score['validation']['AUC']\n",
        "\n",
        "    # Create validation split for optimization\n",
        "    X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=42)\n",
        "\n",
        "    # Suppress verbose output from Optuna\n",
        "    optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
        "\n",
        "    # Optimize hyperparameters\n",
        "    study = optuna.create_study(direction='maximize')\n",
        "    study.optimize(objective, n_trials=5)  # Adjust n_trials for more thorough tuning\n",
        "\n",
        "    return study.best_params"
      ],
      "metadata": {
        "id": "ek1jIRQZHkOl"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble Class**"
      ],
      "metadata": {
        "id": "fR1mavpWKI8s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Ensemble:\n",
        "    \"\"\"Stacking ensemble using XGBoost, LightGBM, and CatBoost as base models,\n",
        "    and logistic regression as the meta-model.\"\"\"\n",
        "\n",
        "    def __init__(self, catboost_params=None):\n",
        "        # Handle class imbalance by scaling the positive class weight\n",
        "        scale_pos_weight = (len(y_train) - sum(y_train)) / sum(y_train)\n",
        "\n",
        "        # Define base models with custom hyperparameters\n",
        "        self.base_models = [\n",
        "            XGBClassifier(\n",
        "                n_estimators=3000,\n",
        "                max_depth=7,\n",
        "                learning_rate=0.015,\n",
        "                gamma=0.2,\n",
        "                subsample=0.6,\n",
        "                colsample_bytree=0.6,\n",
        "                reg_alpha=0.2,\n",
        "                reg_lambda=1.5,\n",
        "                scale_pos_weight=scale_pos_weight,\n",
        "                tree_method='gpu_hist' if torch.cuda.is_available() else 'hist',\n",
        "                eval_metric='auc',\n",
        "                early_stopping_rounds=100,\n",
        "                random_state=42\n",
        "            ),\n",
        "            LGBMClassifier(\n",
        "                n_estimators=3000,\n",
        "                max_depth=6,\n",
        "                learning_rate=0.025,\n",
        "                subsample=0.7,\n",
        "                colsample_bytree=0.7,\n",
        "                reg_alpha=0.2,\n",
        "                reg_lambda=0.2,\n",
        "                scale_pos_weight=scale_pos_weight,\n",
        "                metric='auc',\n",
        "                device='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "                early_stopping_round=100,\n",
        "                random_state=42,\n",
        "                verbosity=-1\n",
        "            ),\n",
        "            # CatBoost uses Optuna-tuned or default hyperparameters\n",
        "            CatBoostClassifier(\n",
        "                **(catboost_params if catboost_params else {\n",
        "                    'iterations': 3000,\n",
        "                    'depth': 7,\n",
        "                    'learning_rate': 0.025,\n",
        "                    'l2_leaf_reg': 3,\n",
        "                    'random_strength': 0.2,\n",
        "                    'scale_pos_weight': scale_pos_weight,\n",
        "                    'grow_policy': 'Lossguide',\n",
        "                    'early_stopping_rounds': 100,\n",
        "                    'task_type': 'GPU' if torch.cuda.is_available() else 'CPU',\n",
        "                    'random_state': 42,\n",
        "                    'verbose': 0,\n",
        "                    'logging_level': 'Silent'\n",
        "                })\n",
        "            )\n",
        "        ]\n",
        "\n",
        "        # Meta-model to combine base model outputs\n",
        "        self.meta_model = LogisticRegression(\n",
        "            penalty='elasticnet',\n",
        "            solver='saga',\n",
        "            l1_ratio=0.5,\n",
        "            C=0.1,\n",
        "            max_iter=1000,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        self.optimal_threshold = None  # Will be set after threshold optimization\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Train base models using out-of-fold predictions and fit meta-model\"\"\"\n",
        "        kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "        meta_features = np.zeros((len(X), len(self.base_models)))  # Store OOF predictions\n",
        "\n",
        "        for i, model in enumerate(self.base_models):\n",
        "            print(f\"\\nTraining base model {i+1}/{len(self.base_models)} ({model.__class__.__name__})...\")\n",
        "            for train_idx, val_idx in kf.split(X, y):\n",
        "                X_tr, X_val = X[train_idx], X[val_idx]\n",
        "                y_tr, y_val = y[train_idx], y[val_idx]\n",
        "\n",
        "                # Fit each model and get validation predictions\n",
        "                if isinstance(model, CatBoostClassifier):\n",
        "                    model.fit(X_tr, y_tr, eval_set=(X_val, y_val), use_best_model=True, verbose=0)\n",
        "                elif isinstance(model, LGBMClassifier):\n",
        "                    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], callbacks=[early_stopping(100), log_evaluation(100)])\n",
        "                else:\n",
        "                    model.fit(X_tr, y_tr, eval_set=[(X_val, y_val)], verbose=False)\n",
        "\n",
        "                # Store probability predictions for validation fold\n",
        "                meta_features[val_idx, i] = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        # Train meta-model on stacked predictions\n",
        "        print(\"\\nTraining meta-model...\")\n",
        "        self.meta_model.fit(meta_features, y)\n",
        "\n",
        "        # Optimize decision threshold based on TPR@FPR metric\n",
        "        y_meta = self.meta_model.predict_proba(meta_features)[:, 1]\n",
        "        self.optimal_threshold = optimize_threshold(y, y_meta)\n",
        "        print(f\"\\nOptimal threshold: {self.optimal_threshold:.4f}\")\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        \"\"\"Get meta-model probability predictions from base model outputs\"\"\"\n",
        "        meta_features = np.column_stack([\n",
        "            model.predict_proba(X)[:, 1] for model in self.base_models\n",
        "        ])\n",
        "        return self.meta_model.predict_proba(meta_features)[:, 1]\n",
        "\n",
        "    def predict(self, X, threshold=None):\n",
        "        \"\"\"Make final predictions using optimized or custom threshold\"\"\"\n",
        "        if threshold is None:\n",
        "            threshold = self.optimal_threshold if self.optimal_threshold else 0.5\n",
        "        return (self.predict_proba(X) >= threshold).astype(int)"
      ],
      "metadata": {
        "id": "nqGhtsmZHkzS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Ensemble Model**"
      ],
      "metadata": {
        "id": "HO5AlhPJKbyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tune CatBoost hyperparameters using Optuna (on same data due to resource limits)\n",
        "print(\"\\nTuning CatBoost hyperparameters\")\n",
        "catboost_params = optimize_catboost(X_scaled, y_train, X_scaled, y_train)\n",
        "\n",
        "# Create and train the stacking ensemble using tuned CatBoost and default XGBoost/LightGBM\n",
        "ensemble = Ensemble(catboost_params=catboost_params)\n",
        "ensemble.fit(X_scaled, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xzvUbFHfHqlw",
        "outputId": "9d3cddf5-523c-46d1-ddc3-f74e9f8655c3"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tuning CatBoost hyperparameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Default metric period is 5 because AUC is/are not implemented for GPU\n",
            "Default metric period is 5 because AUC is/are not implemented for GPU\n",
            "Default metric period is 5 because AUC is/are not implemented for GPU\n",
            "Default metric period is 5 because AUC is/are not implemented for GPU\n",
            "Default metric period is 5 because AUC is/are not implemented for GPU\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training base model 1/3 (XGBClassifier)...\n",
            "\n",
            "Training base model 2/3 (LGBMClassifier)...\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's auc: 0.656048\n",
            "[200]\tvalid_0's auc: 0.65686\n",
            "[300]\tvalid_0's auc: 0.657415\n",
            "[400]\tvalid_0's auc: 0.657726\n",
            "Early stopping, best iteration is:\n",
            "[360]\tvalid_0's auc: 0.659085\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's auc: 0.654498\n",
            "[200]\tvalid_0's auc: 0.653854\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's auc: 0.656769\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's auc: 0.653314\n",
            "[200]\tvalid_0's auc: 0.650569\n",
            "Early stopping, best iteration is:\n",
            "[121]\tvalid_0's auc: 0.654107\n",
            "Training until validation scores don't improve for 100 rounds\n",
            "[100]\tvalid_0's auc: 0.650133\n",
            "\n",
            "Training base model 3/3 (CatBoostClassifier)...\n",
            "\n",
            "Training meta-model...\n",
            "\n",
            "Optimal threshold: 0.6534\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation**"
      ],
      "metadata": {
        "id": "uwkSi1p2Kp-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "y_train_score = ensemble.predict_proba(X_scaled)\n",
        "y_train_pred = ensemble.predict(X_scaled)\n",
        "train_metrics = calculate_metrics(y_train, y_train_pred, y_train_score)\n",
        "\n",
        "print(\"\\nFinal Training Metrics:\")\n",
        "for metric, value in train_metrics.items():\n",
        "    print(f\"{metric}: {value:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "deSey4nlKoAq",
        "outputId": "2aa0b20a-22f5-4724-882e-71f7c052fb74"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Training Metrics:\n",
            "AUC: 0.7269\n",
            "TPR@FPR=0.05: 0.1662\n",
            "TPR@FPR=0.01: 0.0502\n",
            "Best Threshold: 0.5271\n",
            "Accuracy: 0.5400\n",
            "Precision: 0.7869\n",
            "Recall: 0.1097\n",
            "F1: 0.1926\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Process Private Test Data for Inference**"
      ],
      "metadata": {
        "id": "lpzw8FPeK-pK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract features from the private (test) set\n",
        "priv_loader = DataLoader(priv_data, batch_size=64, shuffle=False)\n",
        "X_test, ids_test = [], []\n",
        "\n",
        "for ids, imgs, labels in tqdm(priv_loader, desc=\"Processing private data\"):\n",
        "    imgs = imgs.to(device)\n",
        "    labels = labels.to(device)\n",
        "    features = extract_features(extractor, imgs, labels)\n",
        "\n",
        "    for j in range(len(ids)):\n",
        "        # Prepare input row without conv_features, then add derived + conv features\n",
        "        row = [features[k][j] for k in features if k != 'conv_features']\n",
        "        row = add_derived_features(row, features['conv_features'][j].numpy())\n",
        "        X_test.append(row)\n",
        "        ids_test.append(ids[j].item())\n",
        "\n",
        "# Convert to NumPy array and apply the same feature selection and normalization as training\n",
        "X_test = np.array(X_test)\n",
        "X_test_selected = selector.transform(X_test)\n",
        "X_test_scaled = scaler.transform(X_test_selected)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FvwuAfFdILXu",
        "outputId": "f81e4c3a-4359-4f02-8122-1fdde5a41393"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing private data: 100%|██████████| 313/313 [00:17<00:00, 17.75it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generating Submission\n",
        "\n",
        "membership_scores = ensemble.predict_proba(X_test_scaled)\n",
        "submission_df = pd.DataFrame({\n",
        "    \"ids\": ids_test,\n",
        "    \"score\": np.clip(membership_scores, 0.001, 0.999)\n",
        "})\n",
        "submission_df.to_csv(\"submission.csv\", index=False)"
      ],
      "metadata": {
        "id": "W1F07BwIIIe5"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the trained ensemble model and preprocessing tools\n",
        "joblib.dump(ensemble, \"mia_ensemble.pkl\")   # Stacked ensemble model\n",
        "joblib.dump(scaler, \"scaler.pkl\")           # QuantileTransformer used for scaling\n",
        "joblib.dump(selector, \"selector.pkl\")       # Feature selector (top 50 features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGedGizdKyz4",
        "outputId": "510ab373-8592-4723-888a-5728f605e888"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['selector.pkl']"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    }
  ]
}